{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME+3syW+pJb1PdQUNurJI4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacdono/ml-studies/blob/main/pca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estudo Prático: Análise de Componentes Principais (PCA)\n",
        "\n",
        "Este notebook demonstra o uso do PCA para **redução de dimensionalidade** e **visualização** de dados.\n",
        "\n",
        "Vamos aplicar o PCA a um dataset de dígitos manuscritos, que originalmente possui 64 dimensões (pixels de uma imagem 8x8), e reduzi-lo para apenas 2 dimensões para que possamos visualizá-lo em um gráfico.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4sN6ur_PMbqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso!\")"
      ],
      "metadata": {
        "id": "XalxE07xMgHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# O dataset 'digits' contém imagens de 8x8 pixels de dígitos manuscritos (0-9)\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "print(f\"Formato dos dados (X): {X.shape}\")\n",
        "print(f\"Cada amostra tem {X.shape[1]} features (pixels).\")\n",
        "\n",
        "# Mostrando algumas imagens de exemplo\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5),\n",
        "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
        "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
        "    ax.text(0.05, 0.05, str(digits.target[i]),\n",
        "            transform=ax.transAxes, color='green', fontsize=14)\n",
        "\n",
        "plt.suptitle(\"Exemplos do Dataset 'digits'\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1ny18hR_MlkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### Passo 1: Padronizar os Dados\n",
        "\n",
        "O PCA é sensível à escala das features. Portanto, é uma boa prática padronizar os dados (média 0 e desvio padrão 1) antes de aplicar o PCA.\n",
        "\"\"\"\n",
        "\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "print(\"Dados padronizados com sucesso.\")"
      ],
      "metadata": {
        "id": "RR4hZSvlMoCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### Passo 2: Aplicar o PCA e Analisar a Variância Explicada\n",
        "\n",
        "Primeiro, vamos aplicar o PCA sem definir um número de componentes para ver quanta variância cada componente consegue explicar. Isso nos ajuda a decidir quantos componentes usar.\n",
        "\"\"\"\n",
        "\n",
        "# Aplicando PCA para manter todos os componentes\n",
        "pca_analysis = PCA().fit(X_scaled)\n",
        "\n",
        "# Plotando a variância explicada acumulada\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.cumsum(pca_analysis.explained_variance_ratio_))\n",
        "plt.xlabel('Número de Componentes')\n",
        "plt.ylabel('Variância Explicada Acumulada')\n",
        "plt.title('Análise da Variância Explicada pelo PCA')\n",
        "plt.grid(True)\n",
        "plt.axhline(y=0.95, color='r', linestyle='--', label='95% da Variância')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Observação: Com cerca de 30 componentes, já retemos 95% da informação original.\")"
      ],
      "metadata": {
        "id": "oJ3pAG9cMtbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### Passo 3: Reduzir para 2 Dimensões\n",
        "\n",
        "Agora, vamos aplicar o PCA novamente, mas desta vez especificando `n_components=2` para que possamos plotar os dados em um gráfico 2D.\n",
        "\"\"\"\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"Formato dos dados original: {X_scaled.shape}\")\n",
        "print(f\"Formato dos dados após PCA: {X_pca.shape}\")\n"
      ],
      "metadata": {
        "id": "cBL_UD0aMwcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### Passo 4: Visualizar os Componentes Principais\n",
        "\n",
        "Vamos criar um `DataFrame` com os dois componentes principais e os rótulos originais para facilitar a plotagem. O gráfico mostrará como o PCA conseguiu agrupar os diferentes dígitos mesmo em um espaço de apenas 2 dimensões.\n",
        "\"\"\"\n",
        "\n",
        "# Criando um DataFrame para a visualização\n",
        "df_pca = pd.DataFrame(data=X_pca, columns=['Componente Principal 1', 'Componente Principal 2'])\n",
        "df_pca['target'] = y\n",
        "\n",
        "# Plotando o resultado\n",
        "plt.figure(figsize=(12, 9))\n",
        "sns.scatterplot(\n",
        "    x=\"Componente Principal 1\", y=\"Componente Principal 2\",\n",
        "    hue=\"target\",\n",
        "    palette=sns.color_palette(\"hsv\", 10),\n",
        "    data=df_pca,\n",
        "    legend=\"full\",\n",
        "    alpha=0.8\n",
        ")\n",
        "\n",
        "plt.title('PCA do Dataset de Dígitos (Reduzido para 2 Componentes)')\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "**Conclusão:** Como podemos ver, os dados de 64 dimensões foram projetados em um plano 2D. Mesmo com essa drástica redução, os clusters correspondentes a cada dígito estão, em sua maioria, bem definidos e separados. Isso mostra o poder do PCA em encontrar as direções de maior variância nos dados.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Zy2N-VYdM0QW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}