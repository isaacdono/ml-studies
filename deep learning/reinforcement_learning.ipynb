{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN62hhql+jR9OvDfb5phI4U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacdono/ml-studies/blob/main/deep%20learning/reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estudo Prático: Aprendizado por Reforço com Q-Learning\n",
        "\n",
        "O **Aprendizado por Reforço (Reinforcement Learning - RL)** é uma área do Machine Learning onde um **agente** aprende a tomar **ações** em um **ambiente** para maximizar uma noção de **recompensa** cumulativa.\n",
        "\n",
        "Diferente de outros tipos de aprendizado, não há um \"gabarito\". O agente aprende por tentativa e erro. O ciclo básico é:\n",
        "1.  O agente observa o **estado** atual do ambiente.\n",
        "2.  Com base no estado, o agente escolhe uma **ação**.\n",
        "3.  O ambiente transita para um novo estado e dá ao agente uma **recompensa** (positiva ou negativa).\n",
        "4.  O agente usa essa recompensa para atualizar sua estratégia (política) e repete o processo.\n",
        "\n",
        "Neste notebook, vamos implementar um dos algoritmos mais famosos de RL, o **Q-Learning**, para ensinar um agente a resolver o ambiente \"FrozenLake\" da biblioteca `Gymnasium`."
      ],
      "metadata": {
        "id": "lyIKRfveiZUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gymnasium é um toolkit padrão para desenvolver e comparar algoritmos de RL.\n",
        "# Pygame é necessário para a renderização do ambiente.\n",
        "# Se não os tiver, descomente e execute a linha abaixo.\n",
        "# !pip install gymnasium pygame\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso!\")\n"
      ],
      "metadata": {
        "id": "klBswbXNiaVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "O ambiente `FrozenLake` é um mundo em grade 4x4. O objetivo é navegar do estado inicial (S) até o estado final (G) sem cair nos buracos (H).\n",
        "\n",
        "- S: Start (Início)\n",
        "- F: Frozen (Gelo, seguro para andar)\n",
        "- H: Hole (Buraco, fim de jogo, sem recompensa)\n",
        "- G: Goal (Objetivo, fim de jogo, com recompensa)\n",
        "\n",
        "Vamos usar `is_slippery=False` para tornar o ambiente determinístico, ou seja, se o agente escolher ir para a direita, ele irá para a direita.\n",
        "\"\"\"\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "# Para ver o agente em ação no final, use:\n",
        "# env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"human\")\n",
        "\n",
        "\n",
        "# Obtendo o tamanho do espaço de estados e ações\n",
        "state_space_size = env.observation_space.n\n",
        "action_space_size = env.action_space.n\n",
        "\n",
        "print(f\"Número de Estados: {state_space_size}\")\n",
        "print(f\"Número de Ações: {action_space_size}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Hpkrp2vaidpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "O Q-Learning usa uma **Tabela Q (Q-Table)** para aprender. A tabela tem uma linha para cada estado e uma coluna para cada ação. O valor `Q(s, a)` representa a \"qualidade\" (recompensa futura esperada) de se tomar a ação `a` no estado `s`.\n",
        "\n",
        "A regra de atualização da Tabela Q é:\n",
        "`Q(s, a) <-- Q(s, a) + alpha * [recompensa + gamma * max(Q(s', a')) - Q(s, a)]`\n",
        "\"\"\"\n",
        "\n",
        "# Inicializando a Tabela Q com zeros\n",
        "q_table = np.zeros((state_space_size, action_space_size))\n",
        "\n",
        "# Hiperparâmetros do algoritmo\n",
        "total_episodes = 10000        # Número de jogos que o agente vai jogar\n",
        "learning_rate = 0.1           # Taxa de aprendizado (alpha)\n",
        "discount_factor = 0.99        # Fator de desconto (gamma)\n",
        "\n",
        "# Parâmetros para a estratégia Epsilon-Greedy (Exploration vs Exploitation)\n",
        "epsilon = 1.0                 # Taxa de exploração inicial\n",
        "max_epsilon = 1.0             # Valor máximo de epsilon\n",
        "min_epsilon = 0.01            # Valor mínimo de epsilon\n",
        "epsilon_decay_rate = 0.0005   # Taxa de decaimento de epsilon\n",
        "\n",
        "print(\"Tabela Q e hiperparâmetros inicializados.\")\n",
        "print(\"Formato da Tabela Q:\", q_table.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "ltqRcHE4igGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nIniciando o treinamento...\")\n",
        "time.sleep(1)\n",
        "\n",
        "# Loop principal de treinamento\n",
        "for episode in range(total_episodes):\n",
        "    # Resetar o ambiente para um novo episódio\n",
        "    state, info = env.reset()\n",
        "    terminated = False\n",
        "\n",
        "    while not terminated:\n",
        "        # Dilema Exploração vs. Exploitation (Epsilon-Greedy)\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            # Escolher uma ação aleatória (Exploração)\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # Escolher a melhor ação conhecida (Exploitation)\n",
        "            action = np.argmax(q_table[state, :])\n",
        "\n",
        "        # Executar a ação no ambiente\n",
        "        new_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "        # Atualizar a Tabela Q com a fórmula do Q-Learning\n",
        "        # O agente caiu no buraco ou chegou ao objetivo\n",
        "        if terminated or truncated:\n",
        "            q_table[state, action] = q_table[state, action] + learning_rate * (reward - q_table[state, action])\n",
        "        else:\n",
        "            q_table[state, action] = q_table[state, action] + learning_rate * (\n",
        "                reward + discount_factor * np.max(q_table[new_state, :]) - q_table[state, action]\n",
        "            )\n",
        "\n",
        "        # Atualizar o estado\n",
        "        state = new_state\n",
        "\n",
        "    # Decaimento do Epsilon (para reduzir a exploração ao longo do tempo)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-epsilon_decay_rate * episode)\n",
        "\n",
        "    # Feedback visual do treinamento\n",
        "    if (episode + 1) % 1000 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Treinando... Episódio: {episode + 1}/{total_episodes}\")\n",
        "\n",
        "clear_output(wait=True)\n",
        "print(\"Treinamento concluído!\")\n"
      ],
      "metadata": {
        "id": "-CXA_UcciikF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Após o treinamento, a Tabela Q contém a política ótima aprendida pelo agente. Para cada estado (linha), a coluna com o maior valor indica a melhor ação a ser tomada.\n",
        "\"\"\"\n",
        "print(\"Tabela Q Final (arredondada):\")\n",
        "print(q_table.round(3))\n"
      ],
      "metadata": {
        "id": "v_di2ROWinbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Agora, vamos ver o nosso agente em ação! Vamos executar vários episódios sem exploração (epsilon = 0) e ver com que frequência ele consegue atingir o objetivo.\n",
        "\"\"\"\n",
        "env.close() # Fechar o ambiente anterior para criar um novo com modo de renderização\n",
        "eval_env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"human\")\n",
        "\n",
        "num_eval_episodes = 5\n",
        "total_wins = 0\n",
        "\n",
        "print(\"\\nIniciando a avaliação do agente treinado...\")\n",
        "\n",
        "for episode in range(num_eval_episodes):\n",
        "    state, info = eval_env.reset()\n",
        "    terminated = False\n",
        "    print(f\"\\n--- Episódio de Avaliação #{episode + 1} ---\")\n",
        "    time.sleep(1) # Pausa para visualização\n",
        "\n",
        "    while not terminated:\n",
        "        eval_env.render() # Mostra o estado atual do ambiente\n",
        "\n",
        "        # Escolher a melhor ação da Tabela Q (sempre exploitation)\n",
        "        action = np.argmax(q_table[state, :])\n",
        "\n",
        "        new_state, reward, terminated, truncated, info = eval_env.step(action)\n",
        "        state = new_state\n",
        "        time.sleep(0.3)\n",
        "\n",
        "    eval_env.render() # Renderizar o último estado\n",
        "    if reward == 1.0:\n",
        "        print(\"Agente alcançou o objetivo! (Vitória)\")\n",
        "        total_wins += 1\n",
        "    else:\n",
        "        print(\"Agente caiu em um buraco. (Derrota)\")\n",
        "    time.sleep(1.5)\n",
        "\n",
        "eval_env.close()\n",
        "\n",
        "# Calculando a taxa de sucesso\n",
        "success_rate = (total_wins / num_eval_episodes) * 100\n",
        "print(f\"\\nTaxa de sucesso na avaliação: {success_rate}% ({total_wins}/{num_eval_episodes} vitórias)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "wDeQNKXwipna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### Conclusão\n",
        "\n",
        "Neste notebook, implementamos com sucesso o algoritmo Q-Learning do zero para treinar um agente a resolver um problema clássico de RL.\n",
        "\n",
        "**Principais aprendizados:**\n",
        "1.  **Ciclo do RL:** Vimos na prática o ciclo de `estado -> ação -> recompensa` que fundamenta o aprendizado por reforço.\n",
        "2.  **Tabela Q:** Entendemos como uma simples tabela pode armazenar o \"conhecimento\" do agente sobre o ambiente, mapeando a qualidade de cada par estado-ação.\n",
        "3.  **Exploração vs. Exploitation:** Implementamos a estratégia epsilon-greedy, um conceito crucial em RL que equilibra a necessidade de explorar novas ações e de explorar as melhores ações já conhecidas.\n",
        "\n",
        "O Q-Learning é poderoso, mas limitado a ambientes com espaços de estados e ações discretos e relativamente pequenos.\n",
        "Para problemas mais complexos (como jogos com telas de pixels ou controle de robôs), a Tabela Q se torna inviável.\n",
        "Nesses casos, algoritmos mais avançados como **Deep Q-Networks (DQN)** são usados, onde uma rede neural é treinada\n",
        " para aproximar a função Q, em vez de armazená-la em uma tabela.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NYd-7tHQisYZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}