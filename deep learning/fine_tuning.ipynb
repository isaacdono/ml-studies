{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacdono/ml-studies/blob/main/deep%20learning/fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95WlRId6ArIg"
      },
      "source": [
        "# üìù Guia Pr√°tico de Fine-Tuning\n",
        "\n",
        "Ol√°! Este notebook √© seu guia passo a passo para o mundo do fine-tuning de LLMs. Como estudante de Engenharia de Computa√ß√£o, √© crucial entender n√£o apenas *como* fazer, mas *por que* as t√©cnicas funcionam.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxU74xwH_oTb"
      },
      "source": [
        "As Estrat√©gias de Fine-Tuning\n",
        "\n",
        "1.  **Full Fine-Tuning**:\n",
        "    * **O que √©?** Ajustar *todos* os bilh√µes de pesos do modelo.\n",
        "    * **Problema:** Requer uma quantidade massiva de VRAM (mem√≥ria de GPU), sendo invi√°vel para modelos de 8B em hardware comum ou no Colab. Um modelo de 8B com precis√£o total (32-bit) precisaria de `8B * 4 bytes = 32 GB` de VRAM s√≥ para ser carregado, sem contar a mem√≥ria extra para o treinamento.\n",
        "\n",
        "2.  **PEFT (Parameter-Efficient Fine-Tuning)**:\n",
        "    * **O que √©?** Uma fam√≠lia de t√©cnicas que congela os pesos originais do LLM (que s√£o 99.9% do total) e treina apenas um n√∫mero min√∫sculo de novos par√¢metros \"adaptadores\".\n",
        "    * **Vantagem:** Redu√ß√£o dr√°stica no uso de mem√≥ria e acelera√ß√£o do treino.\n",
        "\n",
        "3.  **LoRA (Low-Rank Adaptation)**:\n",
        "    * **A Estrela do PEFT.** A ideia √© que a \"atualiza√ß√£o\" dos pesos para uma nova tarefa pode ser representada por matrizes de baixo posto (muito menores). Em vez de modificar uma matriz de peso gigante `W`, adicionamos o resultado de duas matrizes pequenas, `A` e `B`, que s√£o trein√°veis (`W' = W + B*A`).\n",
        "\n",
        "4.  **QLoRA (Quantized LoRA)**:\n",
        "    * **A Magia que nos permite rodar no Colab.** √â uma otimiza√ß√£o do LoRA que faz duas coisas geniais:\n",
        "        1.  **Quantiza√ß√£o:** Carrega o modelo principalcom precis√£o reduzida (4-bit em vez de 16-bit), cortando o uso de mem√≥ria em 4x.\n",
        "        2.  **Adapta√ß√£o LoRA:** Treina os adaptadores LoRA normalmente sobre esse modelo quantizado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2UQqxOu_28q"
      },
      "source": [
        "## Parte 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omm0t12iAi-Y"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wlRR6kQbNzK"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the HF_TOKEN from Colab secrets and login\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa43c30b"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"datasnaek/mbti-type\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiPtigwsBGly"
      },
      "outputs": [],
      "source": [
        "# Load the MBTI dataset from the local path\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming the dataset is downloaded to the default kagglehub path\n",
        "# based on the output of the previous download cell.\n",
        "# You might need to adjust this path if you downloaded it elsewhere.\n",
        "dataset_path = \"/root/.cache/kagglehub/datasets/datasnaek/mbti-type/versions/1/mbti_1.csv\"\n",
        "\n",
        "df = pd.read_csv(dataset_path)\n",
        "print(\"Dataset loaded successfully from local path. First 5 records:\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f52d35f0"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Preprocess the dataset\n",
        "def format_mbti_data(examples):\n",
        "    # Create a prompt that includes the MBTI type and the posts\n",
        "    # The model will learn to generate text based on this format.\n",
        "    # We'll use a simple format: \"Personality Type: Posts\"\n",
        "    # You can experiment with different prompt formats.\n",
        "    examples[\"text\"] = [f\"Personality Type: {mbti_type}\\nPosts: {posts}\" for mbti_type, posts in zip(examples[\"type\"], examples[\"posts\"])]\n",
        "    return examples\n",
        "\n",
        "# Apply the formatting function to the DataFrame to create a new column\n",
        "df['text'] = df.apply(lambda row: f\"Personality Type: {row['type']}\\nPosts: {row['posts']}\", axis=1)\n",
        "\n",
        "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
        "# We'll only keep the 'text' column for fine-tuning\n",
        "dataset = Dataset.from_pandas(df[['text']])\n",
        "\n",
        "print(\"Formatted dataset example:\")\n",
        "print(dataset[0]['text'])\n",
        "print(\"\\nDataset structure:\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvUCacl5cThl"
      },
      "outputs": [],
      "source": [
        "# Use a smaller portion of the dataset for faster experimentation\n",
        "# You can adjust the 'frac' parameter to change the percentage of data used.\n",
        "# For example, frac=0.1 uses 10% of the dataset.\n",
        "sample_size = 0.1 # Use 10% of the dataset\n",
        "dataset = dataset.train_test_split(test_size=1-sample_size, seed=42)['train'] # Use a fixed seed for reproducibility\n",
        "\n",
        "print(f\"Using a sampled dataset with {len(dataset)} examples.\")\n",
        "print(\"Sampled dataset structure:\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogwLg0cQBIAo"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Carregando o modelo base usando unsloth\n",
        "max_seq_length = 2048 # You can change this to the maximum length of your sequences\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Enables 4-bit quantization\n",
        "\n",
        "# 1. Load the base model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-2-2b-bnb-4bit\", # Choose your model!\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use provided token to download weights like HF does\n",
        ")\n",
        "\n",
        "# 2. Apply LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number from 8 to 64\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # Recommended for Llama 3\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support Rank Stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")\n",
        "\n",
        "# No need to set pad_token or other configurations manually\n",
        "# unsloth handles this automatically\n",
        "\n",
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXkzg5W_BJuo"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Seus Argumentos do treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./unsloth/gemma-2b-json-finetune\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    max_steps=100,\n",
        "    logging_steps=10,\n",
        "    warmup_ratio=0.05,\n",
        "    # bf16=True, # Unsloth handles this\n",
        "    # optim=\"paged_adamw_8bit\", # Unsloth handles this\n",
        "    push_to_hub=False,\n",
        "    report_to=\"wandb\"\n",
        ")\n",
        "\n",
        "# 3. Passe o 'config' para o SFTTrainer e remova os argumentos antigos\n",
        "trainer = SFTTrainer(\n",
        "    model=model, # Use the unsloth model\n",
        "    tokenizer=tokenizer, # Use the unsloth tokenizer\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        "    dataset_text_field = \"text\", # Specify the text field in your dataset\n",
        "    max_seq_length = max_seq_length,\n",
        ")\n",
        "\n",
        "# Inicia o treinamento\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se em outro momento, voce quiser fazer o uso dos adapters, basta pegar o modelo base e\n",
        "# juntar com os adapters\n",
        "\n",
        "\n",
        "# # 1. Carregando o Modelo Base com unsloth para infer√™ncia\n",
        "# base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name = \"unsloth/gemma-2-2b-bnb-4bit\", # ou o modelo base que voc√™ usou\n",
        "#     max_seq_length = 2048,\n",
        "#     dtype = None,\n",
        "#     load_in_4bit = True, # Carregue em 4 bits para infer√™ncia eficiente\n",
        "# )\n",
        "\n",
        "# # 2. Carregando o Adaptador PEFT e anexando ao modelo base\n",
        "# adapter_path = \"./unsloth/output/adapters\" # Caminho onde voc√™ salvou os adaptadores\n",
        "# peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "# # 3. Preparar o modelo PEFT para infer√™ncia\n",
        "# FastLanguageModel.for_inference(peft_model)\n",
        "\n",
        "# # Agora voc√™ pode usar 'peft_model' para infer√™ncia\n",
        "# # ... (c√≥digo para preparar inputs e gerar texto) ..."
      ],
      "metadata": {
        "id": "DmZIW3iH2Nid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nao eh necessario juntar os adapters aqui, porque o model acabou de ser treinado\n",
        "# e atualizado com as novas informacoes, se quiser usar as informacoes novas em outra\n",
        "# ocasiao sem treinar, deve fazer o procedimento acima\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "# Use the Gemma prompt format\n",
        "test_instruction = \"Uma d√∫vida, se eu sou mais animado, sou INTJ?\"\n",
        "test_input = \"\"\n",
        "gemma_prompt = f\"<start_of_turn>user\\n{test_instruction}\\n{test_input}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "    [gemma_prompt], return_tensors = \"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 30, use_cache = False)\n",
        "print(tokenizer.batch_decode(outputs))"
      ],
      "metadata": {
        "id": "Kdaq9b0wtjl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./unsloth/output/adapters\")  # Local saving\n",
        "tokenizer.save_pretrained(\"./unsloth/output/adapters\")\n",
        "\n",
        "\n",
        "# If you can load the adapter when using the model base now"
      ],
      "metadata": {
        "id": "-pf-mzUIx3fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwAi-t71BqMR"
      },
      "source": [
        "## Saiba Mais\n",
        "\n",
        "\n",
        "Parab√©ns, voc√™ treinou um adaptador LoRA! No entanto, no estado atual, voc√™ tem duas partes: o modelo base e seu pequeno adaptador LoRA. Para fazer uma infer√™ncia, voc√™ precisa carregar ambos.\n",
        "\n",
        "A mesclagem √© o processo de \"fundir\" os pesos do seu adaptador LoRA de volta aos pesos do modelo base. O resultado √© um √∫nico modelo aut√¥nomo que j√° cont√©m a sua especializa√ß√£o.\n",
        "\n",
        "Mas e se voc√™ tivesse treinado **v√°rios adaptadores** LoRA?\n",
        "* Um para aprender sobre personalidades.\n",
        "* Outro para ser um especialista em programa√ß√£o Python.\n",
        "* Um terceiro para escrever de forma criativa.\n",
        "\n",
        "Voc√™ pode mescl√°-los para criar um \"super-modelo\" que faz tudo isso. A fun√ß√£o `.merge_and_unload()` n√£o √© ideal para isso. A ferramenta padr√£o da comunidade √© a **`mergekit`**.\n",
        "\n",
        "`mergekit` funciona com um arquivo de configura√ß√£o YAML onde voc√™ especifica os modelos e a estrat√©gia de mesclagem.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1hNg9bDM8LuOppZHeY81BdUR9PzHePfvU",
      "authorship_tag": "ABX9TyNwsY3E+eLMXI4mringTL9y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}