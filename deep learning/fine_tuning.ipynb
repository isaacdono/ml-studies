{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1hNg9bDM8LuOppZHeY81BdUR9PzHePfvU",
      "authorship_tag": "ABX9TyMSVQIseSkfWWAtAosE/8gL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacdono/ml-studies/blob/main/deep%20learning/fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìù Guia Pr√°tico de Fine-Tuning\n",
        "\n",
        "Ol√°! Este notebook √© seu guia passo a passo para o mundo do fine-tuning de LLMs. Como estudante de Engenharia de Computa√ß√£o, √© crucial entender n√£o apenas *como* fazer, mas *por que* as t√©cnicas funcionam.\n",
        "\n"
      ],
      "metadata": {
        "id": "95WlRId6ArIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As Estrat√©gias de Fine-Tuning\n",
        "\n",
        "1.  **Full Fine-Tuning**:\n",
        "    * **O que √©?** Ajustar *todos* os bilh√µes de pesos do modelo.\n",
        "    * **Problema:** Requer uma quantidade massiva de VRAM (mem√≥ria de GPU), sendo invi√°vel para modelos de 8B em hardware comum ou no Colab. Um modelo de 8B com precis√£o total (32-bit) precisaria de `8B * 4 bytes = 32 GB` de VRAM s√≥ para ser carregado, sem contar a mem√≥ria extra para o treinamento.\n",
        "\n",
        "2.  **PEFT (Parameter-Efficient Fine-Tuning)**:\n",
        "    * **O que √©?** Uma fam√≠lia de t√©cnicas que congela os pesos originais do LLM (que s√£o 99.9% do total) e treina apenas um n√∫mero min√∫sculo de novos par√¢metros \"adaptadores\".\n",
        "    * **Vantagem:** Redu√ß√£o dr√°stica no uso de mem√≥ria e acelera√ß√£o do treino.\n",
        "\n",
        "3.  **LoRA (Low-Rank Adaptation)**:\n",
        "    * **A Estrela do PEFT.** A ideia √© que a \"atualiza√ß√£o\" dos pesos para uma nova tarefa pode ser representada por matrizes de baixo posto (muito menores). Em vez de modificar uma matriz de peso gigante `W`, adicionamos o resultado de duas matrizes pequenas, `A` e `B`, que s√£o trein√°veis (`W' = W + B*A`).\n",
        "\n",
        "4.  **QLoRA (Quantized LoRA)**:\n",
        "    * **A Magia que nos permite rodar no Colab.** √â uma otimiza√ß√£o do LoRA que faz duas coisas geniais:\n",
        "        1.  **Quantiza√ß√£o:** Carrega o modelo principal (Llama 3 8B) com precis√£o reduzida (4-bit em vez de 16-bit), cortando o uso de mem√≥ria em 4x.\n",
        "        2.  **Adapta√ß√£o LoRA:** Treina os adaptadores LoRA normalmente sobre esse modelo quantizado.\n"
      ],
      "metadata": {
        "id": "hxU74xwH_oTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 1"
      ],
      "metadata": {
        "id": "K2UQqxOu_28q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omm0t12iAi-Y"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers accelerate bitsandbytes peft trl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the HF_TOKEN from Colab secrets and login\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "login(token=hf_token)"
      ],
      "metadata": {
        "id": "3wlRR6kQbNzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa43c30b"
      },
      "source": [
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"datasnaek/mbti-type\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MBTI dataset from the local path\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming the dataset is downloaded to the default kagglehub path\n",
        "# based on the output of the previous download cell.\n",
        "# You might need to adjust this path if you downloaded it elsewhere.\n",
        "dataset_path = \"/root/.cache/kagglehub/datasets/datasnaek/mbti-type/versions/1/mbti_1.csv\"\n",
        "\n",
        "df = pd.read_csv(dataset_path)\n",
        "print(\"Dataset loaded successfully from local path. First 5 records:\")\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "DiPtigwsBGly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f52d35f0"
      },
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Preprocess the dataset\n",
        "def format_mbti_data(examples):\n",
        "    # Create a prompt that includes the MBTI type and the posts\n",
        "    # The model will learn to generate text based on this format.\n",
        "    # We'll use a simple format: \"Personality Type: Posts\"\n",
        "    # You can experiment with different prompt formats.\n",
        "    examples[\"text\"] = [f\"Personality Type: {mbti_type}\\nPosts: {posts}\" for mbti_type, posts in zip(examples[\"type\"], examples[\"posts\"])]\n",
        "    return examples\n",
        "\n",
        "# Apply the formatting function to the DataFrame to create a new column\n",
        "df['text'] = df.apply(lambda row: f\"Personality Type: {row['type']}\\nPosts: {row['posts']}\", axis=1)\n",
        "\n",
        "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
        "# We'll only keep the 'text' column for fine-tuning\n",
        "dataset = Dataset.from_pandas(df[['text']])\n",
        "\n",
        "print(\"Formatted dataset example:\")\n",
        "print(dataset[0]['text'])\n",
        "print(\"\\nDataset structure:\")\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a smaller portion of the dataset for faster experimentation\n",
        "# You can adjust the 'frac' parameter to change the percentage of data used.\n",
        "# For example, frac=0.1 uses 10% of the dataset.\n",
        "sample_size = 0.1 # Use 10% of the dataset\n",
        "dataset = dataset.train_test_split(test_size=1-sample_size, seed=42)['train'] # Use a fixed seed for reproducibility\n",
        "\n",
        "print(f\"Using a sampled dataset with {len(dataset)} examples.\")\n",
        "print(\"Sampled dataset structure:\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "gvUCacl5cThl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Carregando o modelo base\n",
        "model_id = \"google/gemma-2b\"\n",
        "\n",
        "# Configura√ß√£o de quantiza√ß√£o (4-bit)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "\n",
        "# Carregando o tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Ativa otimiza√ß√µes para o treino com quantiza√ß√£o\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Configura√ß√£o dos adaptadores LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # \"rank\" da decomposi√ß√£o. Um valor maior treina mais par√¢metros, mas pode levar a overfitting. 16 √© um bom come√ßo.\n",
        "    lora_alpha=16, # Par√¢metro de escala. A regra geral √© que seja 2 * r.\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # Common target modules for decoder models\n",
        "    lora_dropout=0.05, # Dropout para regulariza√ß√£o\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\" # Tarefa de modelagem de linguagem causal\n",
        ")\n",
        "\n",
        "# Aplica o wrapper PEFT no nosso modelo\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Imprime o n√∫mero de par√¢metros trein√°veis para vermos a m√°gica do PEFT\n",
        "peft_model.print_trainable_parameters()\n",
        "# Voc√™ ver√° que o n√∫mero de par√¢metros trein√°veis √© < 1% do total!"
      ],
      "metadata": {
        "id": "ogwLg0cQBIAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Seus Argumentos do treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./google/gemma-2b-json-finetune\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-5,\n",
        "    max_steps=150,\n",
        "    logging_steps=15,\n",
        "    warmup_ratio=0.05,\n",
        "    bf16=True,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"wandb\"\n",
        ")\n",
        "\n",
        "# 3. Passe o 'config' para o SFTTrainer e remova os argumentos antigos\n",
        "trainer = SFTTrainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "# Inicia o treinamento\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "aXkzg5W_BJuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 2\n",
        "\n",
        "Parab√©ns, voc√™ treinou um adaptador LoRA! No entanto, no estado atual, voc√™ tem duas partes: o modelo base gigante (Llama 3 8B) e seu pequeno adaptador LoRA. Para fazer uma infer√™ncia, voc√™ precisa carregar ambos.\n",
        "\n",
        "A **mesclagem** √© o processo de \"fundir\" os pesos do seu adaptador LoRA de volta aos pesos do modelo base. O resultado √© um **√∫nico modelo aut√¥nomo** que j√° cont√©m a sua especializa√ß√£o.\n",
        "\n",
        "### Por que mesclar?\n",
        "\n",
        "1.  **Simplifica√ß√£o de Deploy:** Em vez de gerenciar o modelo base + o adaptador, voc√™ distribui um √∫nico modelo. √â muito mais simples para colocar em produ√ß√£o.\n",
        "2.  **Performance de Infer√™ncia:** A infer√™ncia pode ser ligeiramente mais r√°pida, pois o modelo n√£o precisa mais combinar dinamicamente os pesos do LoRA com os pesos base a cada passada. O c√°lculo `W' = W + B*A` j√° foi feito e \"assado\" no modelo.\n",
        "3.  **Compartilhamento:** Permite que voc√™ compartilhe seu modelo fine-tuned completo na Hugging Face como um novo modelo, e n√£o apenas como um adaptador.\n",
        "\n",
        "Vamos ver como fazer isso na pr√°tica."
      ],
      "metadata": {
        "id": "aLwgk5gQBZg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 11: Carregando o Modelo Base e o Adaptador para Mesclagem\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "# --- Carregando o Modelo Base ---\n",
        "# Desta vez, vamos carregar o modelo em precis√£o reduzida (4-bit)\n",
        "# para a mesclagem, caso a vers√£o de 16-bit exija muita mem√≥ria.\n",
        "\n",
        "model_id = \"google/gemma-2b\"\n",
        "\n",
        "# Configura√ß√£o de quantiza√ß√£o (4-bit)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config, # Use the 4-bit quantization config\n",
        "    device_map=\"auto\", # Explicitly set device_map to auto\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- Carregando o Adaptador PEFT ---\n",
        "# Apontamos para o diret√≥rio onde o `trainer` salvou nosso adaptador LoRA.\n",
        "adapter_path = \"./google/gemma-2b-json-finetune/checkpoint-100\"\n",
        "peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "print(\"Modelo base e adaptador carregados com sucesso.\")"
      ],
      "metadata": {
        "id": "NiaESspxBPWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 12: Executando a Mesclagem e Testando o Modelo Final\n",
        "\n",
        "# Este √© o comando m√°gico que funde os pesos!\n",
        "# Ele \"descarrega\" o wrapper PEFT e retorna um modelo transformers padr√£o.\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "\n",
        "print(\"Mesclagem conclu√≠da!\")\n",
        "\n",
        "# --- Teste do Modelo Mesclado ---\n",
        "# Agora, podemos usar este `merged_model` como qualquer outro modelo da Hugging Face.\n",
        "# Note que n√£o precisamos mais do objeto `peft_model` aqui.\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# O mesmo prompt de teste que usamos antes, formatado manualmente\n",
        "# according to the structure defined in cell DiPtigwsBGly.\n",
        "test_instruction = \"Salveee, como vai? De onde voce eh?\"\n",
        "# Use the Gemma prompt format\n",
        "test_prompt = f\"<start_of_turn>user\\n{test_instruction}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "\n",
        "# Criamos um pipeline com o nosso NOVO modelo mesclado\n",
        "merged_pipe = pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
        "\n",
        "# Executamos a infer√™ncia\n",
        "# Set max_new_tokens to a reasonable value to avoid infinite generation.\n",
        "merged_output = merged_pipe(test_prompt, max_new_tokens=50, do_sample=False)\n",
        "\n",
        "print(\"\\n--- Resposta do Modelo Mesclado e Aut√¥nomo ---\")\n",
        "print(merged_output[0]['generated_text'])\n",
        "\n",
        "# O resultado deve ser id√™ntico ao do modelo com o adaptador PEFT,\n",
        "# provando que a mesclagem foi um sucesso!"
      ],
      "metadata": {
        "id": "-UWv8pxZBdpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 13: Salvando o Modelo Mesclado Completo para o Disco\n",
        "\n",
        "# Agora voc√™ tem um modelo completo. Vamos salv√°-lo.\n",
        "# Este diret√≥rio conter√° todos os arquivos necess√°rios para carregar o modelo\n",
        "# sem precisar do c√≥digo do fine-tuning ou dos adaptadores.\n",
        "\n",
        "output_merged_dir = \"./google/gemma-2b-json-finetune\" # Updated output directory\n",
        "\n",
        "merged_model.save_pretrained(output_merged_dir)\n",
        "tokenizer.save_pretrained(output_merged_dir)\n",
        "\n",
        "print(f\"Modelo mesclado completo salvo em: {output_merged_dir}\")\n",
        "# A partir daqui, voc√™ poderia carregar este modelo com um simples:\n",
        "# AutoModelForCausalLM.from_pretrained(\"./google/gemma-2b-json-finetune\")"
      ],
      "metadata": {
        "id": "ACn47GBYBgJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saiba Mais\n",
        "\n",
        "E se voc√™ tivesse treinado **v√°rios adaptadores** LoRA?\n",
        "* Um para aprender sobre personalidades.\n",
        "* Outro para ser um especialista em programa√ß√£o Python.\n",
        "* Um terceiro para escrever de forma criativa.\n",
        "\n",
        "Voc√™ pode mescl√°-los para criar um \"super-modelo\" que faz tudo isso. A fun√ß√£o `.merge_and_unload()` n√£o √© ideal para isso. A ferramenta padr√£o da comunidade √© a **`mergekit`**.\n",
        "\n",
        "`mergekit` funciona com um arquivo de configura√ß√£o YAML onde voc√™ especifica os modelos e a estrat√©gia de mesclagem.\n"
      ],
      "metadata": {
        "id": "bwAi-t71BqMR"
      }
    }
  ]
}