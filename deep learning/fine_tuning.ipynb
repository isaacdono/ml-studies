{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1hNg9bDM8LuOppZHeY81BdUR9PzHePfvU",
      "authorship_tag": "ABX9TyM71xwDeqNHKUgTA0AAb3aC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacdono/ml-studies/blob/main/deep%20learning/fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìù Guia Pr√°tico de Fine-Tuning: Llama 3 8B com QLoRA\n",
        "\n",
        "Ol√°! Este notebook √© seu guia passo a passo para o mundo do fine-tuning de LLMs. Como estudante de Engenharia de Computa√ß√£o, √© crucial entender n√£o apenas *como* fazer, mas *por que* as t√©cnicas funcionam.\n",
        "\n",
        "### As Estrat√©gias de Fine-Tuning\n",
        "\n",
        "1.  **Full Fine-Tuning**:\n",
        "    * **O que √©?** Ajustar *todos* os bilh√µes de pesos do modelo.\n",
        "    * **Problema:** Requer uma quantidade massiva de VRAM (mem√≥ria de GPU), sendo invi√°vel para modelos de 8B em hardware comum ou no Colab. Um modelo de 8B com precis√£o total (32-bit) precisaria de `8B * 4 bytes = 32 GB` de VRAM s√≥ para ser carregado, sem contar a mem√≥ria extra para o treinamento.\n",
        "\n",
        "2.  **PEFT (Parameter-Efficient Fine-Tuning)**:\n",
        "    * **O que √©?** Uma fam√≠lia de t√©cnicas que congela os pesos originais do LLM (que s√£o 99.9% do total) e treina apenas um n√∫mero min√∫sculo de novos par√¢metros \"adaptadores\".\n",
        "    * **Vantagem:** Redu√ß√£o dr√°stica no uso de mem√≥ria e acelera√ß√£o do treino.\n",
        "\n",
        "3.  **LoRA (Low-Rank Adaptation)**:\n",
        "    * **A Estrela do PEFT.** A ideia √© que a \"atualiza√ß√£o\" dos pesos para uma nova tarefa pode ser representada por matrizes de baixo posto (muito menores). Em vez de modificar uma matriz de peso gigante `W`, adicionamos o resultado de duas matrizes pequenas, `A` e `B`, que s√£o trein√°veis (`W' = W + B*A`).\n",
        "\n",
        "4.  **QLoRA (Quantized LoRA)**:\n",
        "    * **A Magia que nos permite rodar no Colab.** √â uma otimiza√ß√£o do LoRA que faz duas coisas geniais:\n",
        "        1.  **Quantiza√ß√£o:** Carrega o modelo principal (Llama 3 8B) com precis√£o reduzida (4-bit em vez de 16-bit), cortando o uso de mem√≥ria em 4x.\n",
        "        2.  **Adapta√ß√£o LoRA:** Treina os adaptadores LoRA normalmente sobre esse modelo quantizado.\n",
        "\n",
        "**Nosso objetivo hoje:** Vamos ensinar o `Meta Llama 3 8B Instruct` a responder a um comando em linguagem natural com um output em formato JSON bem estruturado, uma tarefa muito √∫til em engenharia de software."
      ],
      "metadata": {
        "id": "95WlRId6ArIg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omm0t12iAi-Y"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install peft\n",
        "!pip install trl\n",
        "!pip install triton\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the HF_TOKEN from Colab secrets and login\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "login(token=hf_token)"
      ],
      "metadata": {
        "id": "3wlRR6kQbNzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para este exemplo, n√£o vamos usar um dataset externo.\n",
        "# Vamos criar um pequeno dataset na m√£o para ensinar o modelo a estruturar sa√≠das em JSON.\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "# Nosso objetivo √© transformar um texto n√£o estruturado sobre um usu√°rio em um JSON.\n",
        "instructions = [\n",
        "    \"Extraia o nome, a idade e a cidade de: 'O usu√°rio Jo√£o Silva tem 28 anos e mora em S√£o Paulo.'\",\n",
        "    \"Transforme em JSON os dados de: 'Ana Souza, 35 anos, residente do Rio de Janeiro.'\",\n",
        "    \"Formate as seguintes informa√ß√µes: 'Carlos Pereira, de Curitiba, tem 42 anos.'\"\n",
        "]\n",
        "\n",
        "outputs = [\n",
        "    '{\"nome\": \"Jo√£o Silva\", \"idade\": 28, \"cidade\": \"S√£o Paulo\"}',\n",
        "    '{\"nome\": \"Ana Souza\", \"idade\": 35, \"cidade\": \"Rio de Janeiro\"}',\n",
        "    '{\"nome\": \"Carlos Pereira\", \"idade\": 42, \"cidade\": \"Curitiba\"}'\n",
        "]\n",
        "\n",
        "# O formato do prompt √© crucial para modelos \"instruct\".\n",
        "# O Mistral tamb√©m usa um formato espec√≠fico.\n",
        "# Vamos formatar nossos dados nesse padr√£o.\n",
        "formatted_data = []\n",
        "for instruction, output in zip(instructions, outputs):\n",
        "    # Mistral prompt format\n",
        "    text = f\"[INST]{instruction}[/INST]{output}\"\n",
        "    formatted_data.append({\"text\": text})\n",
        "\n",
        "# Criando um objeto Dataset da Hugging Face\n",
        "dataset = Dataset.from_dict({\"text\": [item[\"text\"] for item in formatted_data]})\n",
        "\n",
        "print(\"Exemplo de um item do dataset formatado:\")\n",
        "print(dataset[0]['text'])"
      ],
      "metadata": {
        "id": "DiPtigwsBGly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Carregando o modelo base\n",
        "model_id = \"google/gemma-2b\"\n",
        "\n",
        "# Configura√ß√£o de quantiza√ß√£o (4-bit)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "\n",
        "# Carregando o tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Ativa otimiza√ß√µes para o treino com quantiza√ß√£o\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Configura√ß√£o dos adaptadores LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # \"rank\" da decomposi√ß√£o. Um valor maior treina mais par√¢metros, mas pode levar a overfitting. 16 √© um bom come√ßo.\n",
        "    lora_alpha=16, # Par√¢metro de escala. A regra geral √© que seja 2 * r.\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # Common target modules for decoder models\n",
        "    lora_dropout=0.05, # Dropout para regulariza√ß√£o\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\" # Tarefa de modelagem de linguagem causal\n",
        ")\n",
        "\n",
        "# Aplica o wrapper PEFT no nosso modelo\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Imprime o n√∫mero de par√¢metros trein√°veis para vermos a m√°gica do PEFT\n",
        "peft_model.print_trainable_parameters()\n",
        "# Voc√™ ver√° que o n√∫mero de par√¢metros trein√°veis √© < 1% do total!"
      ],
      "metadata": {
        "id": "ogwLg0cQBIAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 7: Execu√ß√£o do Treinamento\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "import bitsandbytes\n",
        "\n",
        "# Argumentos do treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./google/gemma-2b-json-finetune\", # Diret√≥rio para salvar o modelo\n",
        "    per_device_train_batch_size=1, # Batch size pequeno para caber na mem√≥ria\n",
        "    gradient_accumulation_steps=4, # Simula um batch size maior (1*4=4) para estabilizar o treino\n",
        "    learning_rate=2e-4, # Taxa de aprendizado\n",
        "    max_steps=100, # N√∫mero de passos de treino. Com um dataset pequeno, 100 √© suficiente.\n",
        "    logging_steps=10, # Logar o progresso a cada 10 passos\n",
        "    fp16=True, # Usar precis√£o de 16-bit para o treino\n",
        "    # bf16=True, # Uncomment this line if your GPU supports bfloat16\n",
        "    push_to_hub=False,# Set to True to push your model to the Hugging Face Hub\n",
        ")\n",
        "\n",
        "# Criando o objeto Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        "    max_seq_length=512, # Comprimento m√°ximo da sequ√™ncia\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_text_field=\"text\", # O campo do nosso dataset que cont√©m o texto formatado\n",
        "    # packing=True, # Uncomment this line to use packing (more efficient for short sequences)\n",
        ")\n",
        "\n",
        "# Inicia o treinamento\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "aXkzg5W_BJuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Zsu-kwyy0GJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crie uma pasta no seu Drive para guardar os modelos (s√≥ precisa rodar uma vez)\n",
        "!mkdir -p /content/drive/MyDrive/meus_modelos_ic/\n",
        "\n",
        "# Copie a pasta do seu adaptador treinado para o Google Drive\n",
        "!cp -r ./google/gemma-2b-json-finetune/ /content/drive/MyDrive/meus_modelos_ic/\n",
        "print(\"Adaptador salvo com seguran√ßa no Google Drive!\")"
      ],
      "metadata": {
        "id": "diZVdFqy0R10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete peft_model and clear cache to free up memory\n",
        "# del peft_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"peft_model deleted and CUDA cache cleared.\")"
      ],
      "metadata": {
        "id": "O1N4lV6bwcJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete some libraries"
      ],
      "metadata": {
        "id": "YsLldq825vcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß¨ Parte 2: Mesclando (Merging) seu Adaptador LoRA\n",
        "\n",
        "Parab√©ns, voc√™ treinou um adaptador LoRA! No entanto, no estado atual, voc√™ tem duas partes: o modelo base gigante (Llama 3 8B) e seu pequeno adaptador LoRA. Para fazer uma infer√™ncia, voc√™ precisa carregar ambos.\n",
        "\n",
        "A **mesclagem** √© o processo de \"fundir\" os pesos do seu adaptador LoRA de volta aos pesos do modelo base. O resultado √© um **√∫nico modelo aut√¥nomo** que j√° cont√©m a sua especializa√ß√£o.\n",
        "\n",
        "### Por que mesclar?\n",
        "\n",
        "1.  **Simplifica√ß√£o de Deploy:** Em vez de gerenciar o modelo base + o adaptador, voc√™ distribui um √∫nico modelo. √â muito mais simples para colocar em produ√ß√£o.\n",
        "2.  **Performance de Infer√™ncia:** A infer√™ncia pode ser ligeiramente mais r√°pida, pois o modelo n√£o precisa mais combinar dinamicamente os pesos do LoRA com os pesos base a cada passada. O c√°lculo `W' = W + B*A` j√° foi feito e \"assado\" no modelo.\n",
        "3.  **Compartilhamento:** Permite que voc√™ compartilhe seu modelo fine-tuned completo na Hugging Face como um novo modelo, e n√£o apenas como um adaptador.\n",
        "\n",
        "Vamos ver como fazer isso na pr√°tica."
      ],
      "metadata": {
        "id": "aLwgk5gQBZg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Caminho para o adaptador salvo no seu Google Drive\n",
        "adapter_path = \"/content/drive/MyDrive/meus_modelos_ic/google/gemma-2b-json-finetune\""
      ],
      "metadata": {
        "id": "-FuE1Fc82_gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 11: Carregando o Modelo Base e o Adaptador para Mesclagem\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "# --- Carregando o Modelo Base ---\n",
        "# Desta vez, vamos carregar o modelo em precis√£o reduzida (4-bit)\n",
        "# para a mesclagem, caso a vers√£o de 16-bit exija muita mem√≥ria.\n",
        "\n",
        "model_id = \"google/gemma-2b\"\n",
        "\n",
        "# Configura√ß√£o de quantiza√ß√£o (4-bit)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config, # Use the 4-bit quantization config\n",
        "    device_map=\"auto\", # Explicitly set device_map to auto\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- Carregando o Adaptador PEFT ---\n",
        "# Apontamos para o diret√≥rio onde o `trainer` salvou nosso adaptador LoRA.\n",
        "peft_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "print(\"Modelo base e adaptador carregados com sucesso.\")"
      ],
      "metadata": {
        "id": "NiaESspxBPWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 12: Executando a Mesclagem e Testando o Modelo Final\n",
        "\n",
        "# Este √© o comando m√°gico que funde os pesos!\n",
        "# Ele \"descarrega\" o wrapper PEFT e retorna um modelo transformers padr√£o.\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "\n",
        "print(\"Mesclagem conclu√≠da!\")\n",
        "\n",
        "# --- Teste do Modelo Mesclado ---\n",
        "# Agora, podemos usar este `merged_model` como qualquer outro modelo da Hugging Face.\n",
        "# Note que n√£o precisamos mais do objeto `peft_model` aqui.\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# O mesmo prompt de teste que usamos antes\n",
        "test_prompt = tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": \"Extraia as informa√ß√µes de 'Mariana Lima, 25 anos, de Salvador.' em formato JSON.\"}],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Criamos um pipeline com o nosso NOVO modelo mesclado\n",
        "merged_pipe = pipeline(\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
        "\n",
        "# Executamos a infer√™ncia\n",
        "merged_output = merged_pipe(test_prompt, max_new_tokens=50, do_sample=False)\n",
        "\n",
        "print(\"\\n--- Resposta do Modelo Mesclado e Aut√¥nomo ---\")\n",
        "print(merged_output[0]['generated_text'])\n",
        "\n",
        "# O resultado deve ser id√™ntico ao do modelo com o adaptador PEFT,\n",
        "# provando que a mesclagem foi um sucesso!"
      ],
      "metadata": {
        "id": "-UWv8pxZBdpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 13: Salvando o Modelo Mesclado Completo para o Disco\n",
        "\n",
        "# Agora voc√™ tem um modelo completo. Vamos salv√°-lo.\n",
        "# Este diret√≥rio conter√° todos os arquivos necess√°rios para carregar o modelo\n",
        "# sem precisar do c√≥digo do fine-tuning ou dos adaptadores.\n",
        "\n",
        "output_merged_dir = \"./google/gemma-2b-json-finetune\" # Updated output directory\n",
        "\n",
        "merged_model.save_pretrained(output_merged_dir)\n",
        "tokenizer.save_pretrained(output_merged_dir)\n",
        "\n",
        "print(f\"Modelo mesclado completo salvo em: {output_merged_dir}\")\n",
        "# A partir daqui, voc√™ poderia carregar este modelo com um simples:\n",
        "# AutoModelForCausalLM.from_pretrained(\"./google/gemma-2b-json-finetune\")"
      ],
      "metadata": {
        "id": "ACn47GBYBgJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß¨üß™ T√≥pico Avan√ßado: Mesclando M√∫ltiplos Modelos com `mergekit`\n",
        "\n",
        "E se voc√™ tivesse treinado **v√°rios adaptadores** LoRA?\n",
        "* Um para gerar JSON (o que fizemos).\n",
        "* Outro para ser um especialista em programa√ß√£o Python.\n",
        "* Um terceiro para escrever de forma criativa.\n",
        "\n",
        "Voc√™ pode mescl√°-los para criar um \"super-modelo\" que faz tudo isso. A fun√ß√£o `.merge_and_unload()` n√£o √© ideal para isso. A ferramenta padr√£o da comunidade √© a **`mergekit`**.\n",
        "\n",
        "`mergekit` funciona com um arquivo de configura√ß√£o YAML onde voc√™ especifica os modelos e a estrat√©gia de mesclagem.\n",
        "\n",
        "**Exemplo de um arquivo `config.yml` para `mergekit`:**\n",
        "\n",
        "```yaml\n",
        "# Define o modelo base que servir√° de alicerce\n",
        "base_model: meta-llama/Meta-Llama-3-8B-Instruct\n",
        "\n",
        "# Lista os adaptadores LoRA que voc√™ quer mesclar sobre o base\n",
        "slices:\n",
        "  - sources:\n",
        "      # Nosso primeiro adaptador treinado\n",
        "      - model: ./llama3-8b-json-finetune\n",
        "        # Damos um peso positivo para sua contribui√ß√£o\n",
        "        positive_prompt: '{\"nome\": \"Jo√£o\", \"idade\": 30, \"cidade\": \"Qualquer\"}'\n",
        "      # Imagine um segundo adaptador treinado para Python\n",
        "      - model: ./llama3-8b-python-coder-finetune\n",
        "        positive_prompt: 'def hello_world(): print(\"Hello, World!\")'\n",
        "\n",
        "# Define o m√©todo de mesclagem (SLERP √© geralmente melhor que a m√©dia linear)\n",
        "merge_method: ties\n",
        "# Define a precis√£o dos n√∫meros no modelo final\n",
        "dtype: bfloat16"
      ],
      "metadata": {
        "id": "bwAi-t71BqMR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fO_2RPlKCOhq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}