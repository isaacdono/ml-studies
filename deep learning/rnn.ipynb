{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMa+dLJ+zojmn2bn1eKpKuJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacdono/ml-studies/blob/main/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estudo Prático sobre Redes Neurais Recorrentes (RNNs)\n",
        "\n",
        "Bem-vindo a este guia prático sobre RNNs!\n",
        "\n",
        "As **Redes Neurais Recorrentes** são especializadas em processar **dados sequenciais**, como séries temporais, texto ou áudio. Sua principal característica é possuir uma \"memória\" interna (um loop), que permite que informações de passos anteriores persistam e influenciem as saídas futuras.\n",
        "\n",
        "Neste notebook, vamos:\n",
        "1.  **Gerar Dados:** Criar uma série temporal simples (uma onda senoidal).\n",
        "2.  **Pré-processar os Dados:** Preparar os dados para o formato que a RNN espera.\n",
        "3.  **Construir uma RNN Simples:** Usaremos `SimpleRNN` do Keras para prever o próximo ponto da sequência.\n",
        "4.  **Construir uma LSTM:** Implementaremos uma `LSTM` (Long Short-Term Memory), uma RNN mais avançada que lida melhor com dependências de longo prazo.\n",
        "5.  **Avaliar e Comparar:** Visualizaremos as previsões de ambos os modelos para entender seu desempenho."
      ],
      "metadata": {
        "id": "PAh-s9lwKtUf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLZ1v3FKKcWs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(f\"TensorFlow versão: {tf.__version__}\")\n",
        "print(\"Bibliotecas importadas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerando uma onda senoidal\n",
        "t = np.arange(0, 150, 0.1)\n",
        "amplitude = np.sin(t) + np.sin(t*0.5) # Adicionando complexidade\n",
        "\n",
        "# Visualizando os dados\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(t, amplitude)\n",
        "plt.title(\"Série Temporal: Onda Senoidal\")\n",
        "plt.xlabel(\"Tempo\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pVwkJFUyK-kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividindo em treino e teste\n",
        "train_size = int(len(data_scaled) * 0.70)\n",
        "test_size = len(data_scaled) - train_size\n",
        "train_data, test_data = data_scaled[0:train_size, :], data_scaled[train_size:len(data_scaled), :]\n",
        "\n",
        "# Função para criar sequências\n",
        "# A ideia é usar 'n' passos anteriores (look_back) para prever o próximo passo\n",
        "def create_sequences(dataset, look_back=10):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(dataset) - look_back - 1):\n",
        "        a = dataset[i:(i + look_back), 0]\n",
        "        X.append(a)\n",
        "        Y.append(dataset[i + look_back, 0])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "look_back = 20 # Usaremos 20 passos de tempo para prever o 21º\n",
        "X_train, y_train = create_sequences(train_data, look_back)\n",
        "X_test, y_test = create_sequences(test_data, look_back)\n",
        "\n",
        "# As RNNs no Keras esperam a entrada no formato: [amostras, passos_de_tempo, features]\n",
        "# No nosso caso, o número de features é 1.\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "print(f\"Formato de X_train: {X_train.shape}\") # (amostras, timesteps, features)\n",
        "print(f\"Formato de y_train: {y_train.shape}\")"
      ],
      "metadata": {
        "id": "GB7QH9SOLCIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### Modelo 1: SimpleRNN\n",
        "\n",
        "Nossa primeira rede terá uma camada `SimpleRNN` e uma camada `Dense` de saída.\n",
        "\"\"\"\n",
        "tf.random.set_seed(42) # para reprodutibilidade\n",
        "\n",
        "rnn_model = Sequential()\n",
        "rnn_model.add(SimpleRNN(units=50, activation='relu', input_shape=(look_back, 1)))\n",
        "rnn_model.add(Dense(units=1))\n",
        "\n",
        "rnn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "rnn_model.summary()\n",
        "\n",
        "# Treinando o modelo\n",
        "print(\"\\nTreinando a SimpleRNN...\")\n",
        "history_rnn = rnn_model.fit(X_train, y_train, epochs=25, batch_size=32, verbose=1)"
      ],
      "metadata": {
        "id": "OZfdMfXLLHu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### Modelo 2: LSTM (Long Short-Term Memory)\n",
        "\n",
        "A LSTM é uma evolução da RNN que usa \"portões\" (gates) para controlar o fluxo de informação,\n",
        "o que a ajuda a capturar dependências de longo prazo e evitar o problema de desaparecimento do gradiente.\n",
        "A estrutura do modelo é a mesma, apenas trocamos a camada.\n",
        "\"\"\"\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(units=50, activation='relu', input_shape=(look_back, 1)))\n",
        "lstm_model.add(Dense(units=1))\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "lstm_model.summary()\n",
        "\n",
        "# Treinando o modelo\n",
        "print(\"\\nTreinando a LSTM...\")\n",
        "history_lstm = lstm_model.fit(X_train, y_train, epochs=25, batch_size=32, verbose=1)"
      ],
      "metadata": {
        "id": "akSlH8zmLR_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fazendo previsões com ambos os modelos\n",
        "train_predict_rnn = rnn_model.predict(X_train)\n",
        "test_predict_rnn = rnn_model.predict(X_test)\n",
        "\n",
        "train_predict_lstm = lstm_model.predict(X_train)\n",
        "test_predict_lstm = lstm_model.predict(X_test)\n",
        "\n",
        "# Invertendo a normalização para a escala original\n",
        "train_predict_rnn = scaler.inverse_transform(train_predict_rnn)\n",
        "test_predict_rnn = scaler.inverse_transform(test_predict_rnn)\n",
        "train_predict_lstm = scaler.inverse_transform(train_predict_lstm)\n",
        "test_predict_lstm = scaler.inverse_transform(test_predict_lstm)\n",
        "y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Calculando o erro\n",
        "rmse_rnn = np.sqrt(mean_squared_error(y_test_inv, test_predict_rnn))\n",
        "rmse_lstm = np.sqrt(mean_squared_error(y_test_inv, test_predict_lstm))\n",
        "print(f\"\\nRMSE da SimpleRNN no teste: {rmse_rnn:.4f}\")\n",
        "print(f\"RMSE da LSTM no teste:    {rmse_lstm:.4f}\")\n"
      ],
      "metadata": {
        "id": "Vnyg_zlqLaP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### Visualizando as Previsões\n",
        "\n",
        "Vamos plotar os dados originais e as previsões dos nossos modelos para ver como eles se saíram.\n",
        "A linha azul representa os dados reais, enquanto as outras linhas mostram as previsões.\n",
        "\"\"\"\n",
        "# Criando um array vazio para plotar as previsões na posição correta do eixo X\n",
        "plot_data = np.empty_like(data_scaled)\n",
        "plot_data[:, :] = np.nan\n",
        "\n",
        "# Previsões da RNN\n",
        "plot_data[len(train_data)+look_back+1:len(data_scaled)-1, :] = test_predict_rnn\n",
        "\n",
        "# Previsões da LSTM\n",
        "plot_data_lstm = np.empty_like(data_scaled)\n",
        "plot_data_lstm[:, :] = np.nan\n",
        "plot_data_lstm[len(train_data)+look_back+1:len(data_scaled)-1, :] = test_predict_lstm\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 7))\n",
        "plt.plot(scaler.inverse_transform(data_scaled), label=\"Dados Originais\", color='blue')\n",
        "plt.plot(scaler.inverse_transform(plot_data), label=\"Previsão SimpleRNN\", color='orange', linestyle='--')\n",
        "plt.plot(scaler.inverse_transform(plot_data_lstm), label=\"Previsão LSTM\", color='red', linestyle='--')\n",
        "plt.title(\"Comparação: Dados Originais vs. Previsões RNN/LSTM\")\n",
        "plt.xlabel(\"Tempo\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TQQl_7HBLevD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}