{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxntR5u5FCgq1utgPRWmAq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacdono/ml-studies/blob/main/deep%20learning/transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estudo Pr√°tico: Transformers e o Mecanismo de Aten√ß√£o\n",
        "\n",
        "Os Transformers revolucionaram o Processamento de Linguagem Natural (PLN). Diferente das RNNs que processam texto sequencialmente, os Transformers o processam em paralelo. A inova√ß√£o que permitiu isso foi o **Mecanismo de Aten√ß√£o (Attention Mechanism)**, especificamente o **Self-Attention**.\n",
        "\n",
        "**Objetivos deste notebook:**\n",
        "1.  **Entender a Intui√ß√£o:** Por que a aten√ß√£o √© necess√°ria?\n",
        "2.  **Construir do Zero:** Implementar um mecanismo de Self-Attention para entender seu funcionamento interno.\n",
        "3.  **Usar na Pr√°tica:** Aplicar um modelo Transformer real (BERT) da biblioteca Hugging Face para uma tarefa e visualizar seus padr√µes de aten√ß√£o."
      ],
      "metadata": {
        "id": "QqGE22b4gsMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## Parte 1: O Cora√ß√£o do Transformer - Self-Attention do Zero\n",
        "\n",
        "Imagine a frase: \"O rob√¥ pegou a bola porque **ela** era pesada\". Para n√≥s, √© √≥bvio que \"ela\" se refere √† \"bola\". Para um modelo, isso n√£o √© trivial. O Self-Attention permite que o modelo aprenda essas rela√ß√µes, calculando um \"score de aten√ß√£o\" entre cada par de palavras na senten√ßa.\n",
        "\n",
        "Para isso, para cada palavra de entrada, criamos tr√™s vetores:\n",
        "- **Query (Q)**: O que eu estou procurando? (Ex: \"a que 'ela' se refere?\")\n",
        "- **Key (K)**: O que eu tenho a oferecer? (Ex: \"eu sou a 'bola'\")\n",
        "- **Value (V)**: O que eu realmente sou. (A representa√ß√£o da palavra 'bola')\n",
        "\n",
        "O processo √©:\n",
        "1.  Para uma palavra (Query), comparamos seu `Q` com o `K` de todas as outras palavras para encontrar `scores`.\n",
        "2.  Normalizamos os `scores` com uma fun√ß√£o `softmax` para obter pesos de aten√ß√£o.\n",
        "3.  Calculamos uma m√©dia ponderada dos vetores `Value` de todas as palavras, usando os pesos de aten√ß√£o. O resultado √© a nova representa√ß√£o da palavra, rica em contexto.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6lgrPqxmgtuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
        "\n",
        "# 1. Dados de Entrada: Vetores (embeddings) para 3 palavras\n",
        "# Digamos que temos uma frase simples: \"pense sobre m√°quinas\"\n",
        "# Cada palavra tem um embedding de dimens√£o 4.\n",
        "embeddings = np.array([\n",
        "    [1, 0, 1, 0], # pense\n",
        "    [0, 1, 0, 1], # sobre\n",
        "    [1, 1, 0, 1]  # m√°quinas\n",
        "])\n",
        "d_embedding = embeddings.shape[1]\n",
        "\n",
        "# 2. Criar os vetores Q, K, V\n",
        "# Na pr√°tica, s√£o matrizes de pesos trein√°veis. Aqui, vamos defini-las aleatoriamente.\n",
        "# A dimens√£o dos vetores Q, K, V (d_k) pode ser diferente da dimens√£o do embedding.\n",
        "d_k = 3\n",
        "np.random.seed(42)\n",
        "W_query = np.random.rand(d_embedding, d_k)\n",
        "W_key = np.random.rand(d_embedding, d_k)\n",
        "W_value = np.random.rand(d_embedding, d_k)\n",
        "\n",
        "# Calculando Q, K, V\n",
        "Q = np.matmul(embeddings, W_query)\n",
        "K = np.matmul(embeddings, W_key)\n",
        "V = np.matmul(embeddings, W_value)\n",
        "\n",
        "print(\"--- Vetores Q, K, V ---\")\n",
        "print(\"Q:\\n\", Q)\n",
        "print(\"\\nK:\\n\", K)\n",
        "print(\"\\nV:\\n\", V)\n",
        "\n",
        "# 3. Calcular os Scores de Aten√ß√£o\n",
        "# scores = (Q * K^T) / sqrt(d_k)\n",
        "scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
        "print(\"\\n--- Scores Brutos (Antes do Softmax) ---\\n\", scores)\n",
        "\n",
        "# 4. Obter os Pesos de Aten√ß√£o (Softmax)\n",
        "attention_weights = softmax(scores)\n",
        "print(\"\\n--- Pesos de Aten√ß√£o (Ap√≥s Softmax) ---\\n\", attention_weights.round(2))\n",
        "\n",
        "# 5. Calcular a Sa√≠da Final\n",
        "# A sa√≠da para cada palavra √© a soma ponderada dos vetores V de todas as outras palavras.\n",
        "output = np.matmul(attention_weights, V)\n",
        "print(\"\\n--- Sa√≠da Final (Representa√ß√£o Contextualizada) ---\\n\", output)"
      ],
      "metadata": {
        "id": "WSvbMTpsgx2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sentence = [\"pense\", \"sobre\", \"m√°quinas\"]\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(attention_weights, xticklabels=sentence, yticklabels=sentence, annot=True, cmap=\"viridis\")\n",
        "plt.title(\"Visualiza√ß√£o dos Pesos de Self-Attention\")\n",
        "plt.xlabel(\"Palavras (Key)\")\n",
        "plt.ylabel(\"Palavras (Query)\")\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "**An√°lise:** A matriz de calor acima mostra a quem cada palavra \"presta aten√ß√£o\" (incluindo ela mesma). A linha \"pense\" mostra os pesos que ela atribui a \"pense\", \"sobre\" e \"m√°quinas\" para criar sua nova representa√ß√£o.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "V4EHXPwVg1aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## Parte 2: Transformers na Pr√°tica com Hugging Face ü§ó\n",
        "\n",
        "Construir um Transformer completo √© complexo. Felizmente, a biblioteca `transformers` da Hugging Face nos d√° acesso a milhares de modelos pr√©-treinados (como BERT, GPT, T5) com poucas linhas de c√≥digo.\n",
        "\n",
        "A arquitetura completa de um Transformer inclui:\n",
        "- **Multi-Head Attention**: Fazer o Self-Attention v√°rias vezes em paralelo, com diferentes matrizes de pesos, para capturar diferentes tipos de rela√ß√µes.\n",
        "- **Positional Encodings**: Como n√£o h√° recorr√™ncia, injetamos informa√ß√µes sobre a posi√ß√£o das palavras nos embeddings.\n",
        "- **Feed-Forward Networks**: Camadas densas aplicadas a cada posi√ß√£o.\n",
        "- **Encoder-Decoder Stacks**: M√∫ltiplas camadas de encoder e/ou decoder empilhadas.\n",
        "\n",
        "Vamos usar um pipeline para uma tarefa simples: an√°lise de sentimentos.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "hVX23ymWg4dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descomente e execute se n√£o tiver a biblioteca instalada\n",
        "# !pip install transformers torch\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Carregando um pipeline pr√©-treinado para an√°lise de sentimentos\n",
        "# Este modelo foi treinado em um dataset em ingl√™s.\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "print(\"\\n--- An√°lise de Sentimentos ---\")\n",
        "print(\"Frase: 'I love this new movie, it's fantastic!'\")\n",
        "print(\"Resultado:\", sentiment_pipeline(\"I love this new movie, it's fantastic!\"))\n",
        "\n",
        "print(\"\\nFrase: 'The service was terrible from start to finish.'\")\n",
        "print(\"Resultado:\", sentiment_pipeline(\"The service was terrible from start to finish.\"))\n"
      ],
      "metadata": {
        "id": "geOYB5zHg7NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Vamos dar um passo al√©m e visualizar os pesos de aten√ß√£o de um modelo BERT real para entender como ele \"pensa\".\n",
        "\"\"\"\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Usaremos um BERT multilingual para podermos usar uma frase em portugu√™s\n",
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
        "\n",
        "# Frase de exemplo\n",
        "sentence_pt = \"O gato sentou no tapete e o cachorro correu.\"\n",
        "inputs = tokenizer(sentence_pt, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# 'outputs.attentions' √© uma tupla com os pesos de aten√ß√£o de cada camada (12 no BERT-base)\n",
        "# Cada tensor tem o formato: [batch_size, num_heads, sequence_length, sequence_length]\n",
        "attention = outputs.attentions\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "# Vamos visualizar a aten√ß√£o de uma cabe√ßa espec√≠fica (ex: cabe√ßa 0) na primeira camada (camada 0)\n",
        "attention_head_0 = attention[0][0, 0, :, :].detach().numpy()\n",
        "\n",
        "# Plotando a matriz de aten√ß√£o\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(attention_head_0, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\")\n",
        "plt.title(\"Aten√ß√£o de uma Cabe√ßa no BERT (Camada 0, Cabe√ßa 0)\")\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "**An√°lise:** Inspecione o mapa de calor. Note os pesos altos na diagonal (cada palavra presta aten√ß√£o a si mesma) e em tokens especiais como `[CLS]` e `[SEP]`. Observe como \"cachorro\" pode estar prestando mais aten√ß√£o em \"correu\", enquanto \"gato\" presta mais aten√ß√£o em \"sentou\", capturando as rela√ß√µes de contexto da frase. Cada uma das 144 cabe√ßas de aten√ß√£o do BERT (12 camadas x 12 cabe√ßas) aprende a focar em um tipo diferente de rela√ß√£o sint√°tica ou sem√¢ntica.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QScjVrGLg-Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### Conclus√£o\n",
        "\n",
        "Neste notebook, desvendamos o mecanismo de **Self-Attention**, o componente que tornou os Transformers poss√≠veis e mudou o cen√°rio da IA.\n",
        "\n",
        "**Principais aprendizados:**\n",
        "1.  **Aten√ß√£o √© Contexto:** O Self-Attention permite que um modelo pese a import√¢ncia de diferentes palavras em uma sequ√™ncia para construir representa√ß√µes ricas em contexto.\n",
        "2.  **Paralelismo √© Poder:** Ao eliminar a necessidade de processamento sequencial das RNNs, os Transformers podem ser treinados em conjuntos de dados muito maiores e em hardware moderno (GPUs/TPUs) de forma muito mais eficiente.\n",
        "3.  **Modelos de Funda√ß√£o:** A arquitetura Transformer √© a base para quase todos os modelos de linguagem de ponta hoje, incluindo o BERT (focado em entendimento) e a fam√≠lia GPT (focada em gera√ß√£o), que impulsionam aplica√ß√µes como a que voc√™ est√° usando agora.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZaBRJ1NyhBsn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}