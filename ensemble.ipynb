{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPzcLNtqyg1C8x/OHdzRzK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacdono/ml-studies/blob/main/ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estudo Prático sobre Ensemble Methods\n",
        "\n",
        "Olá! Este notebook foi criado para ajudar nos seus estudos sobre **Ensemble Methods** em Machine Learning.\n",
        "\n",
        "Vamos abordar os seguintes pontos:\n",
        "1.  **O que são Ensemble Methods?** Uma breve revisão.\n",
        "2.  **Dataset de Exemplo:** Usaremos um dataset simples para visualizar as fronteiras de decisão.\n",
        "3.  **Modelo Base:** Treinaremos uma única Árvore de Decisão como ponto de comparação.\n",
        "4.  **Bagging:** Implementaremos o `RandomForestClassifier`.\n",
        "5.  **Boosting:** Implementaremos o `GradientBoostingClassifier`.\n",
        "6.  **Comparação:** Analisaremos a performance e as fronteiras de decisão de cada modelo.\n",
        "\n",
        "**Lembre-se:** A ideia central dos métodos de ensemble é combinar múltiplos modelos mais fracos para criar um modelo final mais forte e robusto."
      ],
      "metadata": {
        "id": "RPLCsrLgJQK7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWWjt6sNIxYO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make_moons é ótimo para visualizar problemas de classificação não-linear\n",
        "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
        "\n",
        "# Dividindo os dados em conjuntos de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Visualizando o dataset\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', alpha=0.7)\n",
        "plt.title(\"Dataset 'make_moons'\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3gIkLgKCJfKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_decision_boundary(clf, X, y, title):\n",
        "    \"\"\"Função para plotar a fronteira de decisão de um classificador.\"\"\"\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                         np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=50, edgecolors='k')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Feature 1\")\n",
        "    plt.ylabel(\"Feature 2\")\n"
      ],
      "metadata": {
        "id": "HZ4VhBI4JjZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando e treinando o modelo\n",
        "tree_clf = DecisionTreeClassifier(random_state=42)\n",
        "tree_clf.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões e calculando a acurácia\n",
        "y_pred_tree = tree_clf.predict(X_test)\n",
        "acc_tree = accuracy_score(y_test, y_pred_tree)\n",
        "\n",
        "print(f\"Acurácia da Árvore de Decisão: {acc_tree:.4f}\")\n",
        "\n",
        "# Visualizando a fronteira de decisão\n",
        "plt.figure(figsize=(8, 5))\n",
        "plot_decision_boundary(tree_clf, X, y, f\"Árvore de Decisão (Acurácia: {acc_tree:.2f})\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f6V-8uK2Jrv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### Bagging: Random Forest\n",
        "\n",
        "O Random Forest cria várias árvores de decisão em paralelo, cada uma treinada em um subconjunto aleatório dos dados. A decisão final é tomada pela \"votação\" da maioria das árvores. Isso ajuda a reduzir o sobreajuste (overfitting).\n",
        "\"\"\"\n",
        "\n",
        "# Criando e treinando o modelo\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões e calculando a acurácia\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Acurácia do Random Forest: {acc_rf:.4f}\")\n",
        "\n",
        "# Visualizando a fronteira de decisão\n",
        "plt.figure(figsize=(8, 5))\n",
        "plot_decision_boundary(rf_clf, X, y, f\"Random Forest (Acurácia: {acc_rf:.2f})\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ywypEXKyJt6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### Boosting: Gradient Boosting\n",
        "\n",
        "O Gradient Boosting também cria várias árvores, mas de forma sequencial. Cada nova árvore é treinada para corrigir os erros da árvore anterior. Isso resulta em um modelo muito poderoso, mas que pode ser mais sensível a ruídos.\n",
        "\"\"\"\n",
        "\n",
        "# Criando e treinando o modelo\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Fazendo previsões e calculando a acurácia\n",
        "y_pred_gb = gb_clf.predict(X_test)\n",
        "acc_gb = accuracy_score(y_test, y_pred_gb)\n",
        "\n",
        "print(f\"Acurácia do Gradient Boosting: {acc_gb:.4f}\")\n",
        "\n",
        "# Visualizando a fronteira de decisão\n",
        "plt.figure(figsize=(8, 5))\n",
        "plot_decision_boundary(gb_clf, X, y, f\"Gradient Boosting (Acurácia: {acc_gb:.2f})\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RIEUvUYBJ0lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### Comparação dos Modelos\n",
        "\n",
        "Vamos comparar as acurácias e visualizar as três fronteiras de decisão lado a lado para entender as diferenças.\n",
        "\n",
        "**Observação:** Note como as fronteiras de decisão dos modelos de ensemble são mais \"suaves\" e generalistas do que a da árvore de decisão única, que tende a criar \"caixas\" rígidas e pode se ajustar demais aos dados de treino.\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\"*30)\n",
        "print(\"      Resultados Finais\")\n",
        "print(\"=\"*30)\n",
        "print(f\"Acurácia da Árvore de Decisão: {acc_tree:.4f}\")\n",
        "print(f\"Acurácia do Random Forest:      {acc_rf:.4f}\")\n",
        "print(f\"Acurácia do Gradient Boosting:  {acc_gb:.4f}\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Plotando tudo lado a lado\n",
        "fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
        "\n",
        "# Árvore de Decisão\n",
        "plt.sca(axes[0])\n",
        "plot_decision_boundary(tree_clf, X, y, f\"Árvore de Decisão (Acc: {acc_tree:.2f})\")\n",
        "\n",
        "# Random Forest\n",
        "plt.sca(axes[1])\n",
        "plot_decision_boundary(rf_clf, X, y, f\"Random Forest (Acc: {acc_rf:.2f})\")\n",
        "\n",
        "# Gradient Boosting\n",
        "plt.sca(axes[2])\n",
        "plot_decision_boundary(gb_clf, X, y, f\"Gradient Boosting (Acc: {acc_gb:.2f})\")\n",
        "\n",
        "plt.suptitle(\"Comparação das Fronteiras de Decisão\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C2ClIkd7J8Pw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}